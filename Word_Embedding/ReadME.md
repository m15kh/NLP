# Word Embedding

In this project, we implement word embeddings in multiple ways:
- **Pretrained Word Embeddings**: Using pre-trained models like Word2Vec, GloVe, or FastText.
- **Training Word Embeddings**: Training custom word embeddings on a specific dataset.
- **From-Scratch Implementation**: Building word embeddings from scratch to understand the underlying concepts.

This repository provides code and explanations for each of these approaches.